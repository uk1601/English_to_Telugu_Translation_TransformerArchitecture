{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference - Language Translation using Transformer Architecture: English to Telugu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the Trained Transformer Model from Checkpoint using the below hyperparameters as the Model and Tokenizers are saved in the below format.\n",
    "\n",
    "*training_model_name_to_save = f\"{time_str}_{embedding_dim}_{fully_connected_dim}_{num_layers}_{num_heads}_{dropout_rate}_{input_vocab_size}_{target_vocab_size}_{EPOCHS}_{batch_size}_{MAX_LEN}\"*\n",
    "\n",
    "* Load the Tokenizer by loading path ---> tokenizer_{training_model_name_to_save}.pickle\n",
    "* Load the Model by loading the path ---> transformer_{training_model_name_to_save}\n",
    "\n",
    "##### For Inference, Along with **Greedy Search**, Implemented the **Beam-Search** (beam-width is the hyperparameter to select top-k predicted tokens).\n",
    "\n",
    "* Hyperparameters:\n",
    "\n",
    "    * embedding_dim = 256          # dimensionality of the embeddings used for tokens in the input and target sequences\n",
    "    * fully_connected_dim = 512    # dimensionality of the hidden layer of the feedforward neural network within the Transformer block\n",
    "    * num_layers = 4               # number of Transformer blocks in the encoder and decoder stacks\n",
    "    * num_heads = 8                # number of heads in the multi-head attention mechanism\n",
    "    * dropout_rate = 0.1           # dropout rate for regularization\n",
    "\n",
    "    * input_vocab_size = 20394    \n",
    "    * target_vocab_size = 32926   \n",
    "\n",
    "    * max_positional_encoding_input = 20394    # maximum positional encoding value for input sequence\n",
    "    * max_positional_encoding_target = 32926  # maximum positional encoding value for target sequence\n",
    "\n",
    "    * EPOCHS = 120\n",
    "    * batch_size = 32\n",
    "\n",
    "    * MAX_LEN = 325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:06:06.643829Z",
     "iopub.status.busy": "2023-04-15T04:06:06.642314Z",
     "iopub.status.idle": "2023-04-15T04:06:11.983782Z",
     "shell.execute_reply": "2023-04-15T04:06:11.982497Z",
     "shell.execute_reply.started": "2023-04-15T04:06:06.643776Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os \n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LayerNormalization\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:13.789062Z",
     "iopub.status.busy": "2023-04-15T04:04:13.788076Z",
     "iopub.status.idle": "2023-04-15T04:04:14.373134Z",
     "shell.execute_reply": "2023-04-15T04:04:14.372068Z",
     "shell.execute_reply.started": "2023-04-15T04:04:13.789001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>telugu_instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>telugu_output</th>\n",
       "      <th>instruction_len</th>\n",
       "      <th>output_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td>ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి.</td>\n",
       "      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n",
       "      <td>1. సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజన...</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td>మూడు ప్రాధమిక రంగులు ఏమిటి?</td>\n",
       "      <td>The three primary colors are red, blue, and ye...</td>\n",
       "      <td>మూడు ప్రాధమిక రంగులు ఎరుపు, నీలం మరియు పసుపు. ...</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td>పరమాణువు నిర్మాణాన్ని వివరించండి.</td>\n",
       "      <td>An atom is the basic building block of all mat...</td>\n",
       "      <td>పరమాణువు అనేది అన్ని పదార్ధాల యొక్క ప్రాథమిక బ...</td>\n",
       "      <td>6</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td>వాయు కాలుష్యాన్ని ఎలా తగ్గించవచ్చు?</td>\n",
       "      <td>There are several ways to reduce air pollution...</td>\n",
       "      <td>వాయు కాలుష్యాన్ని తగ్గించడానికి అనేక మార్గాలు ...</td>\n",
       "      <td>6</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretend you are a project manager of a constru...</td>\n",
       "      <td>మీరు ఒక కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేన...</td>\n",
       "      <td>I had to make a difficult decision when I was ...</td>\n",
       "      <td>ఓ కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేనేజర్ గ...</td>\n",
       "      <td>21</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0               Give three tips for staying healthy.   \n",
       "1                 What are the three primary colors?   \n",
       "2                 Describe the structure of an atom.   \n",
       "3                   How can we reduce air pollution?   \n",
       "4  Pretend you are a project manager of a constru...   \n",
       "\n",
       "                                  telugu_instruction  \\\n",
       "0         ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి.   \n",
       "1                        మూడు ప్రాధమిక రంగులు ఏమిటి?   \n",
       "2                  పరమాణువు నిర్మాణాన్ని వివరించండి.   \n",
       "3                వాయు కాలుష్యాన్ని ఎలా తగ్గించవచ్చు?   \n",
       "4  మీరు ఒక కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేన...   \n",
       "\n",
       "                                              output  \\\n",
       "0  1. Eat a balanced and nutritious diet: Make su...   \n",
       "1  The three primary colors are red, blue, and ye...   \n",
       "2  An atom is the basic building block of all mat...   \n",
       "3  There are several ways to reduce air pollution...   \n",
       "4  I had to make a difficult decision when I was ...   \n",
       "\n",
       "                                       telugu_output  instruction_len  \\\n",
       "0  1. సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజన...                6   \n",
       "1  మూడు ప్రాధమిక రంగులు ఎరుపు, నీలం మరియు పసుపు. ...                6   \n",
       "2  పరమాణువు అనేది అన్ని పదార్ధాల యొక్క ప్రాథమిక బ...                6   \n",
       "3  వాయు కాలుష్యాన్ని తగ్గించడానికి అనేక మార్గాలు ...                6   \n",
       "4  ఓ కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేనేజర్ గ...               21   \n",
       "\n",
       "   output_len  \n",
       "0         121  \n",
       "1          53  \n",
       "2         209  \n",
       "3         216  \n",
       "4         133  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../artifacts/yahma_alpaca_cleaned_telugu_filtered_and_romanized.csv')\n",
    "\n",
    "cols = [\"instruction\", \"telugu_instruction\", \"output\", \"telugu_output\"]\n",
    "df = df[cols]\n",
    "df['instruction_len'] = df['instruction'].apply(lambda x: len(x.split()))\n",
    "df['output_len'] = df[\"output\"].apply(lambda x: len(x.split()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>telugu_instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>telugu_output</th>\n",
       "      <th>instruction_len</th>\n",
       "      <th>output_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>give three tips for staying healthy</td>\n",
       "      <td>ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి</td>\n",
       "      <td>. eat a balanced and nutritious diet: make sur...</td>\n",
       "      <td>. సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజనం...</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the three primary colors?</td>\n",
       "      <td>మూడు ప్రాధమిక రంగులు ఏమిటి?</td>\n",
       "      <td>the three primary colors are red COMMA blue CO...</td>\n",
       "      <td>మూడు ప్రాధమిక రంగులు ఎరుపు COMMA నీలం మరియు పస...</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>describe the structure of an atom</td>\n",
       "      <td>పరమాణువు నిర్మాణాన్ని వివరించండి</td>\n",
       "      <td>an atom is the basic building block of all mat...</td>\n",
       "      <td>పరమాణువు అనేది అన్ని పదార్ధాల యొక్క ప్రాథమిక బ...</td>\n",
       "      <td>6</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how can we reduce air pollution?</td>\n",
       "      <td>వాయు కాలుష్యాన్ని ఎలా తగ్గించవచ్చు?</td>\n",
       "      <td>there are several ways to reduce air pollution...</td>\n",
       "      <td>వాయు కాలుష్యాన్ని తగ్గించడానికి అనేక మార్గాలు ...</td>\n",
       "      <td>6</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretend you are a project manager of a constru...</td>\n",
       "      <td>మీరు ఒక కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేన...</td>\n",
       "      <td>i had to make a difficult decision when i was ...</td>\n",
       "      <td>ఓ కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేనేజర్ గ...</td>\n",
       "      <td>21</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0                give three tips for staying healthy   \n",
       "1                 what are the three primary colors?   \n",
       "2                  describe the structure of an atom   \n",
       "3                   how can we reduce air pollution?   \n",
       "4  pretend you are a project manager of a constru...   \n",
       "\n",
       "                                  telugu_instruction  \\\n",
       "0          ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి   \n",
       "1                        మూడు ప్రాధమిక రంగులు ఏమిటి?   \n",
       "2                   పరమాణువు నిర్మాణాన్ని వివరించండి   \n",
       "3                వాయు కాలుష్యాన్ని ఎలా తగ్గించవచ్చు?   \n",
       "4  మీరు ఒక కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేన...   \n",
       "\n",
       "                                              output  \\\n",
       "0  . eat a balanced and nutritious diet: make sur...   \n",
       "1  the three primary colors are red COMMA blue CO...   \n",
       "2  an atom is the basic building block of all mat...   \n",
       "3  there are several ways to reduce air pollution...   \n",
       "4  i had to make a difficult decision when i was ...   \n",
       "\n",
       "                                       telugu_output  instruction_len  \\\n",
       "0  . సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజనం...                6   \n",
       "1  మూడు ప్రాధమిక రంగులు ఎరుపు COMMA నీలం మరియు పస...                6   \n",
       "2  పరమాణువు అనేది అన్ని పదార్ధాల యొక్క ప్రాథమిక బ...                6   \n",
       "3  వాయు కాలుష్యాన్ని తగ్గించడానికి అనేక మార్గాలు ...                6   \n",
       "4  ఓ కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేనేజర్ గ...               21   \n",
       "\n",
       "   output_len  \n",
       "0         121  \n",
       "1          53  \n",
       "2         209  \n",
       "3         216  \n",
       "4         133  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def preprocess_text(text, is_telugu=False):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove end-of-line periods and specific telugu punctuation\n",
    "    text = re.sub(\"\\.$\", '', text)  # English and Telugu common\n",
    "\n",
    "    if is_telugu:\n",
    "        text = re.sub(\"。$\", '', text)  # Telugu-specific\n",
    "    \n",
    "    # Handle punctuation (add spaces or replace based on the case)\n",
    "    text = re.sub(r\"([!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"['\\\"]\", '', text)  # Remove quotes directly\n",
    "    text = text.replace(\",\", ' COMMA')  # Handle commas specifically\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Normalize spacing (e.g., after removing or altering punctuation)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['instruction'] = df['instruction'].apply(lambda x: preprocess_text(x))\n",
    "df['telugu_instruction'] = df['telugu_instruction'].apply(lambda x: preprocess_text(x, is_telugu=True))\n",
    "\n",
    "df['output'] = df['output'].apply(lambda x: preprocess_text(x))\n",
    "df['telugu_output'] = df['telugu_output'].apply(lambda x: preprocess_text(x, is_telugu=True))\n",
    "# Review a sample\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Name and Parameter Initialization\n",
    "Load the Tokenizer and Model from Checkpoints using: \n",
    "*training_model_name_to_save = f\"{time_str}_{embedding_dim}_{fully_connected_dim}_{num_layers}_{num_heads}_{dropout_rate}_{input_vocab_size}_{target_vocab_size}_{EPOCHS}_{batch_size}_{MAX_LEN}\"*\n",
    "\n",
    "* Tokenizer - tokenizer_{training_model_name_to_save}.pickle\n",
    "* Model - transformer_{training_model_name_to_save}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2024-04-03-04-28_256_512_4_8_0.1_20394_32926_50_32_325\"\n",
    "\n",
    "# \"2024-04-02-03-10_256_512_4_8_0.1_9566_14126_50_32_312\"\n",
    "# training_model_name_to_save = f\"{time_str}_{embedding_dim}_{fully_connected_dim}_{num_layers}_{num_heads}_{dropout_rate}_{input_vocab_size}_{target_vocab_size}_{EPOCHS}_{batch_size}_{MAX_LEN}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for the Transformer model\n",
    "embedding_dim = int(model_name.split(\"_\")[-10])         \n",
    "fully_connected_dim = int(model_name.split(\"_\")[-9])\n",
    "num_layers = int(model_name.split(\"_\")[-8])              \n",
    "num_heads = int(model_name.split(\"_\")[-7] )                \n",
    "dropout_rate = float(model_name.split(\"_\")[-6])\n",
    "\n",
    "input_vocab_size = int(model_name.split(\"_\")[-5])\n",
    "target_vocab_size = int(model_name.split(\"_\")[-4])\n",
    "\n",
    "max_positional_encoding_input = input_vocab_size\n",
    "max_positional_encoding_target = target_vocab_size\n",
    "\n",
    "EPOCHS = int(model_name.split(\"_\")[-3])\n",
    "batch_size = int(model_name.split(\"_\")[-2])\n",
    "MAX_LEN = int(model_name.split(\"_\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 0.1, 32926)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim, dropout_rate, target_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:24.795636Z",
     "iopub.status.busy": "2023-04-15T04:04:24.795192Z",
     "iopub.status.idle": "2023-04-15T04:04:24.808604Z",
     "shell.execute_reply": "2023-04-15T04:04:24.806731Z",
     "shell.execute_reply.started": "2023-04-15T04:04:24.795595Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, embedding_dim):\n",
    "    \"\"\"\n",
    "    Function to compute the angles for positional encoding.\n",
    "    \n",
    "    Returns the angle computed\n",
    "    \"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(embedding_dim))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, embedding_dim):\n",
    "    \"\"\"\n",
    "    Adds  positional encoding to the Embeddings to be fed to the Transformer model.\n",
    "    \n",
    "    Computes a sin and cos of the angles determined by the get_angles() function\n",
    "    and adds the value computed to an axis of the embeddings.\n",
    "    \"\"\"\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n",
    "                           np.arange(embedding_dim)[np.newaxis, :], embedding_dim)\n",
    "    \n",
    "    # apply sin to even indices in the array. ie 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array. ie 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:25.683644Z",
     "iopub.status.busy": "2023-04-15T04:04:25.682870Z",
     "iopub.status.idle": "2023-04-15T04:04:25.690752Z",
     "shell.execute_reply": "2023-04-15T04:04:25.689156Z",
     "shell.execute_reply.started": "2023-04-15T04:04:25.683594Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a padding mask for a given sequence.\n",
    "    \n",
    "    Args:\n",
    "        seq (tensor): A tensor of shape (batch_size, seq_len) containing the sequence.\n",
    "        \n",
    "    Returns:\n",
    "        A tensor of shape (batch_size, 1, 1, seq_len) containing a mask that is 1 where the sequence is padded, and 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Convert the sequence to a boolean tensor where True indicates a pad token (value 0).\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # Add an extra dimension to the mask to add the padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a look-ahead mask used during training the decoder of a transformer.\n",
    "\n",
    "    Args:\n",
    "        size (int): The size of the mask.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A lower triangular matrix of shape (size, size) with ones on the diagonal\n",
    "            and zeros below the diagonal.\n",
    "    \"\"\"\n",
    "    # create a matrix with ones on the diagonal and zeros below the diagonal\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def create_masks(inputs, targets):\n",
    "    \"\"\"\n",
    "    Creates masks for the input sequence and target sequence.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input sequence tensor.\n",
    "        targets: Target sequence tensor.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of three masks: the encoder padding mask, the combined mask used in the first attention block, \n",
    "        and the decoder padding mask used in the second attention block.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the encoder padding mask.\n",
    "    enc_padding_mask = create_padding_mask(inputs)\n",
    "        \n",
    "    # Create the decoder padding mask.\n",
    "    dec_padding_mask = create_padding_mask(inputs)\n",
    "        \n",
    "    # Create the look ahead mask for the first attention block.\n",
    "    # It is used to pad and mask future tokens in the tokens received by the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
    "    \n",
    "    # Create the decoder target padding mask.\n",
    "    dec_target_padding_mask = create_padding_mask(targets)\n",
    "    \n",
    "    # Combine the look ahead mask and decoder target padding mask for the first attention block.\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism - Multihead Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:25.723314Z",
     "iopub.status.busy": "2023-04-15T04:04:25.722884Z",
     "iopub.status.idle": "2023-04-15T04:04:25.734660Z",
     "shell.execute_reply": "2023-04-15T04:04:25.733356Z",
     "shell.execute_reply.started": "2023-04-15T04:04:25.723273Z"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot product attention weight for the query (q), key (k), and value (v) vectors. \n",
    "    The attention weight is a measure of how much focus should be given to each element in the sequence of values (v) \n",
    "    based on the corresponding element in the sequence of queries (q) and keys (k).\n",
    "    \n",
    "    Args:\n",
    "    q: query vectors; shape (..., seq_len_q, depth)\n",
    "    k: key vectors; shape  (..., seq_len_k, depth)\n",
    "    v: value vectors; shape  (..., seq_len_v, depth_v)\n",
    "    mask: (optional) mask to be applied to the attention weights\n",
    "    \n",
    "    Returns:\n",
    "    output: The output of the scaled dot product attention computation; shape   (..., seq_len_q, depth_v)\n",
    "    attention_weights: The attention weights\n",
    "    \"\"\"\n",
    "    # Compute dot product of query and key vectors\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Compute the square root of the depth of the key vectors\n",
    "    dk = tf.cast(tf.shape(k)[-1], dtype=tf.float32)\n",
    "    scaled_dk = tf.math.sqrt(dk)\n",
    "    \n",
    "    # Compute scaled attention logits by dividing dot product by scaled dk\n",
    "    scaled_attention_logits = matmul_qk / scaled_dk\n",
    "    \n",
    "    # Apply mask to the attention logits (if mask is not None)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    # Apply softmax to the scaled attention logits to get the attention weights\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    # Compute the weighted sum of the value vectors using the attention weights\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:25.737261Z",
     "iopub.status.busy": "2023-04-15T04:04:25.736831Z",
     "iopub.status.idle": "2023-04-15T04:04:25.756663Z",
     "shell.execute_reply": "2023-04-15T04:04:25.755160Z",
     "shell.execute_reply.started": "2023-04-15T04:04:25.737221Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    MultiHeadAttention Layer that implements the attention mechanism for the Transformer.\n",
    "    It splits the input into multiple heads, computes scaled dot-product attention for each head\n",
    "    and then concatenates the output of the heads and passes it through a dense layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, key_dim, num_heads, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadAttention layer.\n",
    "    \n",
    "        Args:\n",
    "            key_dim (int): The dimensionality of the key space.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate to apply after the dense layer.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        #  ensure  that the dimension of the embedding can be evenly split across attention heads\n",
    "        assert key_dim % num_heads == 0 \n",
    "        self.depth = self.key_dim // self.num_heads\n",
    "        \n",
    "        # dense layers to project the input into queries, keys and values\n",
    "        self.wq = Dense(key_dim)\n",
    "        self.wk = Dense(key_dim)\n",
    "        self.wv = Dense(key_dim)\n",
    "    \n",
    "        # dropout layer\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "        # dense layer to project the output of the attention heads\n",
    "        self.dense = Dense(key_dim)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Splits the last dimension of the tensor into (num_heads, depth).\n",
    "        Transposes the result such that the shape is (batch_size, num_heads, seq_len, depth).\n",
    "    \n",
    "        Args:\n",
    "            x (tensor): The tensor to be split.\n",
    "            batch_size (int): The size of the batch.\n",
    "    \n",
    "        Returns:\n",
    "            tensor: The tensor with the last dimension split into (num_heads, depth) and transposed.\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        \"\"\"\n",
    "        Applies the multi-head attention mechanism to the inputs.\n",
    "    \n",
    "        Args:\n",
    "            v (tensor): The value tensor of shape (batch_size, seq_len_v, key_dim).\n",
    "            k (tensor): The key tensor of shape (batch_size, seq_len_k, key_dim).\n",
    "            q (tensor): The query tensor of shape (batch_size, seq_len_q, key_dim).\n",
    "            mask (tensor, optional): The mask tensor of shape (batch_size, seq_len_q, seq_len_k).\n",
    "                                     Defaults to None.\n",
    "    \n",
    "        Returns:\n",
    "            tensor: The output tensor of shape (batch_size, seq_len_q, key_dim).\n",
    "            tensor: The attention weights tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k).\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # Dense on the q, k, v vectors\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # split the heads\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # split the queries, keys and values into multiple heads\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # reshape and add Dense layer\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
    "        output = self.dense(concat_attention)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "\n",
    "### Fully Connected NeuralNetwork\n",
    "    \n",
    "def FeedForward(embedding_dim, fully_connected_dim):\n",
    "    \"\"\"Create a fully connected feedforward neural network.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim (int): Dimensionality of the embedding output from the transformer layer.\n",
    "        fully_connected_dim (int): Number of neurons in the fully connected layers.\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Sequential: A fully connected feedforward neural network with the specified architecture.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(embedding_dim)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:25.776663Z",
     "iopub.status.busy": "2023-04-15T04:04:25.775928Z",
     "iopub.status.idle": "2023-04-15T04:04:25.792270Z",
     "shell.execute_reply": "2023-04-15T04:04:25.790775Z",
     "shell.execute_reply.started": "2023-04-15T04:04:25.776608Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
    "        \"\"\"Initializes the encoder layer\n",
    "        \n",
    "        Args: \n",
    "            embedding_dim: The dimensionality of the input and output of this layer\n",
    "            num_heads: The number of attention heads to use in the multi-head attention layer\n",
    "            fully_connected_dim: The dimensionality of the hidden layer in the feedforward network\n",
    "            dropout_rate: The rate of dropout to apply to the output of this layer during training\n",
    "            \n",
    "        Returns:\n",
    "            A new instance of the EncoderLayer class\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention mechanism\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.ffn = FeedForward(embedding_dim, fully_connected_dim)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"Applies the encoder layer to the input tensor\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor to the encoder layer\n",
    "            training: A boolean indicating whether the model is in training mode\n",
    "            mask: A tensor representing the mask to apply to the attention mechanism\n",
    "            \n",
    "        Returns:\n",
    "            The output of the encoder layer after applying the multi-head attention and feedforward network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply multi-head self-attention mechanism to input tensor\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        \n",
    "        # Apply first layer normalization and add residual connection\n",
    "        out1 = self.layernorm1(attn_output + x)\n",
    "        \n",
    "        # Apply feedforward network to output of first layer normalization\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "        \n",
    "        # Apply second layer normalization and add residual connection\n",
    "        out2 = self.layernorm2(ffn_output + out1)\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "\n",
    "#### Encoder \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder layer of the Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            num_layers (int): Number of EncoderLayers to stack.\n",
    "            embedding_dim (int): Dimensionality of the token embedding space.\n",
    "            num_heads (int): Number of attention heads to use in MultiHeadAttention layers.\n",
    "            fully_connected_dim (int): Dimensionality of the fully connected layer in the EncoderLayer.\n",
    "            input_vocab_size (int): Size of the input vocabulary.\n",
    "            maximum_position_encoding (int): Maximum length of input sequences for positional encoding.\n",
    "            dropout_rate (float): Probability of dropping out units during training.\n",
    "\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = Embedding(input_vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate) for _ in range(num_layers)]\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        \"\"\"\n",
    "        Call function for the Encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            inputs: tensor of shape (batch_size, sequence_length) representing input sequences\n",
    "            training: boolean indicating if the model is in training mode\n",
    "            mask: tensor of shape (batch_size, sequence_length) representing the mask to apply to the input sequence\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, sequence_length, embedding_dim) representing the encoded sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the sequence length\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        # Embed the input sequence\n",
    "        inputs = self.embedding(inputs)\n",
    "\n",
    "        # Scale the embeddings by sqrt(embedding_dim)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # Add positional encodings to the input sequence\n",
    "        inputs += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Apply dropout to the input sequence\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "\n",
    "        # Pass the input sequence through the encoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i](inputs, training, mask)\n",
    "\n",
    "        # Return the encoded sequence\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:25.813154Z",
     "iopub.status.busy": "2023-04-15T04:04:25.812693Z",
     "iopub.status.idle": "2023-04-15T04:04:25.829594Z",
     "shell.execute_reply": "2023-04-15T04:04:25.828047Z",
     "shell.execute_reply.started": "2023-04-15T04:04:25.813098Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes a single decoder layer of the transformer model.\n",
    "        \n",
    "        Args:\n",
    "        embedding_dim: The dimension of the embedding space.\n",
    "        num_heads: The number of attention heads to use.\n",
    "        fully_connected_dim: The dimension of the feedforward network.\n",
    "        rate: The dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Instantiate two instances of MultiHeadAttention.\n",
    "        self.mha1 = MultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
    "        self.mha2 = MultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
    "        \n",
    "        # Instantiate a fully connected feedforward network.\n",
    "        self.ffn = FeedForward(embedding_dim, fully_connected_dim)\n",
    "        \n",
    "        # Instantiate three layer normalization layers with epsilon=1e-6.\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Instantiate a dropout layer for regularization.\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder layer.\n",
    "        \n",
    "        Args:\n",
    "        x: The input to the decoder layer, a query vector.\n",
    "        enc_output: The output from the top layer of the encoder, a set of attention vectors k and v.\n",
    "        training: Whether to apply dropout regularization.\n",
    "        look_ahead_mask: The mask to apply to the input sequence so that it can't look ahead to future positions.\n",
    "        padding_mask: The mask to apply to the input sequence to ignore padding tokens.\n",
    "        \n",
    "        Returns:\n",
    "        The output from the decoder layer, a tensor with the same shape as the input.\n",
    "        The attention weights from the first multi-head attention layer.\n",
    "        The attention weights from the second multi-head attention layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply the first multi-head attention layer to the query vector x.\n",
    "        # We pass x as all three inputs to the layer because this is a self-attention layer.\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        \n",
    "        # Add the original input to the output of the attention layer and apply layer normalization.\n",
    "        out1 = self.layernorm1(attn1 + x) \n",
    "        \n",
    "        # Apply the second multi-head attention layer to the output from the first layer and the encoder output.\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        \n",
    "        # Add the output from the first layer to the output of the second layer and apply layer normalization.\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        # Apply the feedforward network to the output of the second layer and apply dropout regularization.\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        # Add the output from the second layer to the output of the feedforward network and apply layer normalization.\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "### Decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder object.\n",
    "        \n",
    "        Args:\n",
    "            num_layers (int): The number of Decoder layers.\n",
    "            embedding_dim (int): The size of the embedding dimension.\n",
    "            num_heads (int): The number of heads in the MultiHeadAttention layer.\n",
    "            fully_connected_dim (int): The number of units in the feedforward network.\n",
    "            target_vocab_size (int): The number of words in the target vocabulary.\n",
    "            maximum_position_encoding (int): The maximum length of a sequence.\n",
    "            dropout_rate (float): The rate at which to apply dropout.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # create layers\n",
    "        self.embedding = Embedding(target_vocab_size, embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, embedding_dim)\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Executes the Decoder.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): The input to the Decoder.\n",
    "            enc_output (tf.Tensor): The output from the Encoder.\n",
    "            training (bool): Whether the Decoder is in training mode.\n",
    "            look_ahead_mask (tf.Tensor): The mask for self-attention in the MultiHeadAttention layer.\n",
    "            padding_mask (tf.Tensor): The mask for padding in the MultiHeadAttention layer.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output from the Decoder.\n",
    "            dict: A dictionary of attention weights.\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        # add embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # apply each layer of the decoder\n",
    "        for i in range(self.num_layers):\n",
    "            # pass through decoder layer i\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            # record attention weights for block1 and block2\n",
    "            attention_weights[f\"decoder_layer{i + 1}_block1\"] = block1\n",
    "            attention_weights[f\"decoder_layer{i + 1}_block2\"] = block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:25.854463Z",
     "iopub.status.busy": "2023-04-15T04:04:25.853940Z",
     "iopub.status.idle": "2023-04-15T04:04:25.869228Z",
     "shell.execute_reply": "2023-04-15T04:04:25.868169Z",
     "shell.execute_reply.started": "2023-04-15T04:04:25.854402Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A Transformer model that takes in an input and target sequence and outputs a final prediction.\n",
    "\n",
    "    Args:\n",
    "        num_layers (int): Number of layers in the Encoder and Decoder.\n",
    "        embedding_dim (int): Dimensionality of the embedding layer.\n",
    "        num_heads (int): Number of attention heads used in the Transformer.\n",
    "        fully_connected_dim (int): Dimensionality of the fully connected layer in the Encoder and Decoder.\n",
    "        input_vocab_size (int): Size of the input vocabulary.\n",
    "        target_vocab_size (int): Size of the target vocabulary.\n",
    "        max_positional_encoding_input (int): Maximum length of the input sequence.\n",
    "        max_positional_encoding_target (int): Maximum length of the target sequence.\n",
    "        dropout_rate (float, optional): Dropout rate used in the Encoder and Decoder layers. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, max_positional_encoding_input, max_positional_encoding_target, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Initialize the Encoder and Decoder layers\n",
    "        self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_positional_encoding_input, dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_positional_encoding_target, dropout_rate)\n",
    "        \n",
    "        # Add a final dense layer to make the final prediction\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            inp (tf.Tensor): Input sequence tensor with shape (batch_size, input_seq_len).\n",
    "            tar (tf.Tensor): Target sequence tensor with shape (batch_size, target_seq_len).\n",
    "            training (bool): Whether the model is being trained or not.\n",
    "            enc_padding_mask (tf.Tensor): Padding mask for the Encoder with shape (batch_size, 1, 1, input_seq_len).\n",
    "            look_ahead_mask (tf.Tensor): Mask to prevent the Decoder from looking ahead in the target sequence with shape (batch_size, 1, target_seq_len, target_seq_len).\n",
    "            dec_padding_mask (tf.Tensor): Padding mask for the Decoder with shape (batch_size, 1, 1, target_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the final output of the model and the attention weights of the Decoder.\n",
    "        \"\"\"\n",
    "        # Pass the input sequence through the Encoder\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        # Pass the target sequence and the output of the Encoder through the Decoder\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # Pass the output of the Decoder through the final dense layer to get the final prediction\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer - Learning Rate Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:04:29.910025Z",
     "iopub.status.busy": "2023-04-15T04:04:29.909177Z",
     "iopub.status.idle": "2023-04-15T04:04:29.920888Z",
     "shell.execute_reply": "2023-04-15T04:04:29.919328Z",
     "shell.execute_reply.started": "2023-04-15T04:04:29.909968Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embedding_dim, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.embedding_dim = tf.cast(embedding_dim, dtype=tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embedding_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# Create an instance of the custom learning rate schedule\n",
    "learning_rate = CustomSchedule(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:11:33.617982Z",
     "iopub.status.busy": "2023-04-15T04:11:33.617524Z",
     "iopub.status.idle": "2023-04-15T04:11:33.989559Z",
     "shell.execute_reply": "2023-04-15T04:11:33.987797Z",
     "shell.execute_reply.started": "2023-04-15T04:11:33.617941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Projects\\telugu-chatbot\\tcvenv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:585: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2 = 0.98, epsilon = 1e-9)\n",
    "\n",
    "# Define the loss object\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "def loss_function(true_values, predictions):\n",
    "    # Create a mask to exclude the padding tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(true_values, 0))\n",
    "\n",
    "    # Compute the loss value using the loss object\n",
    "    loss_ = loss_object(true_values, predictions)\n",
    "\n",
    "    # Apply the mask to exclude the padding tokens\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    # Calculate the mean loss value\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(true_values, predictions):\n",
    "    # Compute the accuracies using the true and predicted target sequences\n",
    "    accuracies = tf.equal(true_values, tf.argmax(predictions, axis=2))\n",
    "\n",
    "    # Create a mask to exclude the padding tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(true_values, 0))\n",
    "\n",
    "    # Apply the mask to exclude the padding tokens from the accuracies\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "\n",
    "    # Calculate the mean accuracy value\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "# Define the training metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Transformer model\n",
    "transformer = Transformer(num_layers, embedding_dim, num_heads,\n",
    "                           fully_connected_dim, input_vocab_size, target_vocab_size, \n",
    "                           max_positional_encoding_input, max_positional_encoding_target, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T04:24:28.196098Z",
     "iopub.status.busy": "2023-04-15T04:24:28.194879Z",
     "iopub.status.idle": "2023-04-15T04:24:28.207235Z",
     "shell.execute_reply": "2023-04-15T04:24:28.205631Z",
     "shell.execute_reply.started": "2023-04-15T04:24:28.196040Z"
    }
   },
   "outputs": [],
   "source": [
    "# the train function\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(encoder_input, target):\n",
    "    # Slice the target tensor to get the input for the decoder\n",
    "    decoder_input = target[:, :-1]\n",
    "\n",
    "    # Slice the target tensor to get the expected output of the decoder\n",
    "    expected_output = target[:, 1:]\n",
    "\n",
    "    # Create masks for the encoder input, decoder input and the padding\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n",
    "\n",
    "    # Perform a forward pass through the model\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(encoder_input, decoder_input, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "\n",
    "        # Calculate the loss between the predicted output and the expected output\n",
    "        loss = loss_function(expected_output, predictions)\n",
    "\n",
    "    # Calculate gradients and update the model parameters\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # Update the training loss and accuracy metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(expected_output, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Reload\n",
    "\n",
    "### Checkpoint Restoration\n",
    "- Load the Model from Checkpoints by using the above hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Checkpoints from: directory/transformer_training_artifacts/training_checkpoints/transformer_2024-04-03-04-28_256_512_4_8_0.1_20394_32926_50_32_325\n",
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = f\"directory/transformer_training_artifacts/training_checkpoints/transformer_{model_name}\"\n",
    "\n",
    "print(f\"Loading the Checkpoints from: {checkpoint_path}\")\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Restoration\n",
    "- Load the Tokenizer from the artifacts directory using the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer\n",
    "import pickle\n",
    "\n",
    "def load_tokenizers(file_path):\n",
    "    # Load the pickled dictionary containing tokenizers\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        tokenizers_dict = pickle.load(handle)\n",
    "    return tokenizers_dict\n",
    "\n",
    "# Assuming file_path contains the path to the pickled file\n",
    "tokenizers_file_path = f'directory/transformer_training_artifacts/tokenizer/tokenizer_{model_name}.pickle'\n",
    "\n",
    "# Load the tokenizers\n",
    "loaded_tokenizers_dict = load_tokenizers(tokenizers_file_path)\n",
    "\n",
    "# Access individual tokenizers by their keys\n",
    "tokenizer_tar = loaded_tokenizers_dict['english_tokenizer_target']\n",
    "tokenizer_inp = loaded_tokenizers_dict['telugu_tokenizer_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-15T04:10:48.639081Z",
     "iopub.status.idle": "2023-04-15T04:10:48.639784Z",
     "shell.execute_reply": "2023-04-15T04:10:48.639463Z",
     "shell.execute_reply.started": "2023-04-15T04:10:48.639424Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = MAX_LEN\n",
    "def translate_helper(sentence):\n",
    "    \"\"\"\n",
    "    Evaluate function that generates a translated sentence from the given input sentence.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): The input sentence in the source language.\n",
    "\n",
    "    Returns:\n",
    "    A tensor representing the translated sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess the input sentence\n",
    "    sentence = preprocess_text(sentence[0], is_telugu=True)\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>' # Add start and end of sentence markers\n",
    "    sentence = [sentence] # Convert sentence to list because of TensorFlow's tokenizer\n",
    "    \n",
    "    # Vectorize and pad the sentence\n",
    "    sentence = tokenizer_inp.texts_to_sequences(sentence)\n",
    "    sentence = pad_sequences(sentence, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    input = tf.convert_to_tensor(np.array(sentence),dtype=tf.int64) # Convert input to tensor\n",
    "    \n",
    "    # Tokenize the start of the decoder input and convert it to a tensor\n",
    "    decoder_input = tokenizer_tar.texts_to_sequences(['<start>'])\n",
    "    decoder_input = tf.convert_to_tensor(np.array(decoder_input), dtype=tf.int64)\n",
    "    \n",
    "    # Generate the translated sentence\n",
    "    for i in range(maxlen):\n",
    "        # Create masks for the encoder, decoder, and combined\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input, decoder_input)\n",
    "        # Generate predictions for the current input sequence\n",
    "        predictions, _ = transformer(input, decoder_input,False,enc_padding_mask,combined_mask, dec_padding_mask)\n",
    "        # Select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :] \n",
    "        # Get the predicted word ID by taking the argmax of the predictions\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int64)\n",
    "        \n",
    "        # If the predicted ID is equal to the end token, return the decoder input\n",
    "        if predicted_id == tokenizer_tar.texts_to_sequences(['<end>']):\n",
    "            return tf.squeeze(decoder_input, axis=0)\n",
    "        \n",
    "        # Concatenate the predicted ID to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        decoder_input = tf.concat([decoder_input, predicted_id], axis=1)\n",
    "    \n",
    "    # Return the translated sentence\n",
    "    return tf.squeeze(decoder_input, axis=0)\n",
    "\n",
    "\n",
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Translate function that generates a translation for the given input sentence.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): The input sentence in the source language.\n",
    "\n",
    "    Returns:\n",
    "    None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert sentence to list because our evaluate function requires lists\n",
    "    sentence = [sentence]\n",
    "    \n",
    "    # Print the input sentence\n",
    "    print(f'Input sentence: {sentence[0]}')\n",
    "    print()\n",
    "    \n",
    "    # Generate the translated sentence\n",
    "    result = (translate_helper(sentence)).tolist()\n",
    "    \n",
    "    # Convert the result tensor to a list of IDs and remove start and end of sentence markers\n",
    "    predicted_ids = [i for i in result if i != tokenizer_tar.texts_to_sequences(['<start>'])[0][0]\n",
    "                     and i != tokenizer_tar.texts_to_sequences(['<end>'])[0][0]]\n",
    "    \n",
    "    # Convert the predicted IDs to a list of words\n",
    "    predicted_sentence = tokenizer_tar.sequences_to_texts([predicted_ids])\n",
    "    \n",
    "    # Print the predicted translation\n",
    "    print(f'Translation: {predicted_sentence[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEAM Search\n",
    "- Beam-width (k) is a hyperparameter, which helps us to choose top-k elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-12T12:19:54.060633Z",
     "iopub.status.idle": "2023-04-12T12:19:54.061136Z",
     "shell.execute_reply": "2023-04-12T12:19:54.060941Z",
     "shell.execute_reply.started": "2023-04-12T12:19:54.060913Z"
    }
   },
   "outputs": [],
   "source": [
    "beam_width = 5  # Define the beam width\n",
    "\n",
    "def translate_helper_beam_search(sentence):\n",
    "    \"\"\"\n",
    "    Evaluate function that generates a translated sentence from the given input sentence using beam search.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): The input sentence in the source language.\n",
    "\n",
    "    Returns:\n",
    "    A tensor representing the translated sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess the input sentence\n",
    "    sentence = preprocess_text(sentence[0], is_telugu=True)\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>' # Add start and end of sentence markers\n",
    "    sentence = [sentence] # Convert sentence to list because of TensorFlow's tokenizer\n",
    "    \n",
    "    # Vectorize and pad the sentence\n",
    "    sentence = tokenizer_inp.texts_to_sequences(sentence)\n",
    "    sentence = pad_sequences(sentence, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    input = tf.convert_to_tensor(np.array(sentence), dtype=tf.int64) # Convert input to tensor\n",
    "    \n",
    "    # Tokenize the start of the decoder input and convert it to a tensor\n",
    "    decoder_input = tokenizer_tar.texts_to_sequences(['<start>'])\n",
    "    decoder_input = tf.convert_to_tensor(np.array(decoder_input), dtype=tf.int64)\n",
    "    \n",
    "    # Initialize the beam search candidates\n",
    "    candidates = [(decoder_input, 0)]  # List of (decoder_input, score) tuples\n",
    "    \n",
    "    # Generate the translated sentence using beam search\n",
    "    for _ in range(maxlen):\n",
    "        new_candidates = []\n",
    "        for decoder_input, score in candidates:\n",
    "            # Create masks for the encoder, decoder, and combined\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input, decoder_input)\n",
    "            # Generate predictions for the current input sequence\n",
    "            predictions, _ = transformer(input, decoder_input, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "            # Select the last word from the seq_len dimension\n",
    "            predictions = predictions[:, -1:, :]\n",
    "            # Get the top beam_width predictions and their indices\n",
    "            topk_probs, topk_ids = tf.nn.top_k(tf.squeeze(predictions, axis=1), k=beam_width)\n",
    "            for i in range(beam_width):\n",
    "                new_decoder_input = tf.concat([decoder_input, tf.expand_dims(tf.expand_dims(tf.cast(topk_ids[0][i], tf.int64), axis=0), axis=0)], axis=1)\n",
    "                new_score = score + tf.math.log(topk_probs[0][i]).numpy()\n",
    "                new_candidates.append((new_decoder_input, new_score))\n",
    "\n",
    "#             for i in range(beam_width):\n",
    "#                 new_decoder_input = tf.concat([decoder_input, tf.expand_dims(topk_ids[0][i], axis=0)], axis=1)\n",
    "#                 new_score = score + tf.math.log(topk_probs[0][i]).numpy()\n",
    "#                 new_candidates.append((new_decoder_input, new_score))\n",
    "        \n",
    "    \n",
    "        # Select the top beam_width candidates\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        # Check if any of the candidates end with the end token\n",
    "        for candidate, _ in candidates:\n",
    "            if candidate[0][-1] == tokenizer_tar.texts_to_sequences(['<end>']):\n",
    "                return tf.squeeze(candidate, axis=0)\n",
    "    \n",
    "    # Return the translated sentence with the highest score among the candidates\n",
    "    return tf.squeeze(candidates[0][0], axis=0)\n",
    "\n",
    "\n",
    "def translate_beam(sentence):\n",
    "    \"\"\"\n",
    "    Translate function that generates a translation for the given input sentence.\n",
    "\n",
    "    Args:\n",
    "    sentence (str): The input sentence in the source language.\n",
    "\n",
    "    Returns:\n",
    "    None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert sentence to list because our evaluate function requires lists\n",
    "    sentence = [sentence]\n",
    "    \n",
    "    # Print the input sentence\n",
    "    print(f'Input sentence: {sentence[0]}')\n",
    "    print()\n",
    "    \n",
    "    # Generate the translated sentence\n",
    "    result = (translate_helper_beam_search(sentence)).tolist()\n",
    "    \n",
    "    # Convert the result tensor to a list of IDs and remove start and end of sentence markers\n",
    "    predicted_ids = [i for i in result if i != tokenizer_tar.texts_to_sequences(['<start>'])[0][0]\n",
    "                     and i != tokenizer_tar.texts_to_sequences(['<end>'])[0][0]]\n",
    "    \n",
    "    # Convert the predicted IDs to a list of words\n",
    "    predicted_sentence = tokenizer_tar.sequences_to_texts([predicted_ids])\n",
    "    \n",
    "    # Print the predicted translation\n",
    "    print(f'Translation: {predicted_sentence[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>telugu_instruction</th>\n",
       "      <th>output</th>\n",
       "      <th>telugu_output</th>\n",
       "      <th>instruction_len</th>\n",
       "      <th>output_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>give three tips for staying healthy</td>\n",
       "      <td>ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి</td>\n",
       "      <td>. eat a balanced and nutritious diet: make sur...</td>\n",
       "      <td>. సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజనం...</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are the three primary colors?</td>\n",
       "      <td>మూడు ప్రాధమిక రంగులు ఏమిటి?</td>\n",
       "      <td>the three primary colors are red COMMA blue CO...</td>\n",
       "      <td>మూడు ప్రాధమిక రంగులు ఎరుపు COMMA నీలం మరియు పస...</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>describe the structure of an atom</td>\n",
       "      <td>పరమాణువు నిర్మాణాన్ని వివరించండి</td>\n",
       "      <td>an atom is the basic building block of all mat...</td>\n",
       "      <td>పరమాణువు అనేది అన్ని పదార్ధాల యొక్క ప్రాథమిక బ...</td>\n",
       "      <td>6</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how can we reduce air pollution?</td>\n",
       "      <td>వాయు కాలుష్యాన్ని ఎలా తగ్గించవచ్చు?</td>\n",
       "      <td>there are several ways to reduce air pollution...</td>\n",
       "      <td>వాయు కాలుష్యాన్ని తగ్గించడానికి అనేక మార్గాలు ...</td>\n",
       "      <td>6</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretend you are a project manager of a constru...</td>\n",
       "      <td>మీరు ఒక కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేన...</td>\n",
       "      <td>i had to make a difficult decision when i was ...</td>\n",
       "      <td>ఓ కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేనేజర్ గ...</td>\n",
       "      <td>21</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0                give three tips for staying healthy   \n",
       "1                 what are the three primary colors?   \n",
       "2                  describe the structure of an atom   \n",
       "3                   how can we reduce air pollution?   \n",
       "4  pretend you are a project manager of a constru...   \n",
       "\n",
       "                                  telugu_instruction  \\\n",
       "0          ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి   \n",
       "1                        మూడు ప్రాధమిక రంగులు ఏమిటి?   \n",
       "2                   పరమాణువు నిర్మాణాన్ని వివరించండి   \n",
       "3                వాయు కాలుష్యాన్ని ఎలా తగ్గించవచ్చు?   \n",
       "4  మీరు ఒక కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేన...   \n",
       "\n",
       "                                              output  \\\n",
       "0  . eat a balanced and nutritious diet: make sur...   \n",
       "1  the three primary colors are red COMMA blue CO...   \n",
       "2  an atom is the basic building block of all mat...   \n",
       "3  there are several ways to reduce air pollution...   \n",
       "4  i had to make a difficult decision when i was ...   \n",
       "\n",
       "                                       telugu_output  instruction_len  \\\n",
       "0  . సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజనం...                6   \n",
       "1  మూడు ప్రాధమిక రంగులు ఎరుపు COMMA నీలం మరియు పస...                6   \n",
       "2  పరమాణువు అనేది అన్ని పదార్ధాల యొక్క ప్రాథమిక బ...                6   \n",
       "3  వాయు కాలుష్యాన్ని తగ్గించడానికి అనేక మార్గాలు ...                6   \n",
       "4  ఓ కన్ స్ట్రక్షన్ కంపెనీలో ప్రాజెక్ట్ మేనేజర్ గ...               21   \n",
       "\n",
       "   output_len  \n",
       "0         121  \n",
       "1          53  \n",
       "2         209  \n",
       "3         216  \n",
       "4         133  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-12T12:19:54.063144Z",
     "iopub.status.idle": "2023-04-12T12:19:54.064270Z",
     "shell.execute_reply": "2023-04-12T12:19:54.064056Z",
     "shell.execute_reply.started": "2023-04-12T12:19:54.064028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search Output:\n",
      "Input sentence: field of autonomous vehicles is rapidly evolving with numerous advancements being made in recent years\n",
      "\n",
      "Translation: వాహనాల రంగం వేగంగా అభివృద్ధి చెందుతోంది ఇటీవలి సంవత్సరాలలో అనేక పురోగతులు తాజా పరిణామాల్లో కొన్ని\n",
      "\n",
      "\n",
      "Beam Search Output:\n",
      "Input sentence: field of autonomous vehicles is rapidly evolving with numerous advancements being made in recent years\n",
      "\n",
      "Translation: వాహనాల రంగం వేగంగా అభివృద్ధి చెందుతోంది ఇటీవలి సంవత్సరాలలో అత్యంత తాజా పరిణామాల్లో కొన్ని\n"
     ]
    }
   ],
   "source": [
    "t = \"field of autonomous vehicles is rapidly evolving with numerous advancements being made in recent years\"\n",
    "\n",
    "print(f\"Greedy Search Output:\")\n",
    "translate(t)\n",
    "print(f\"\\n\\nBeam Search Output:\")\n",
    "translate_beam(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T16:55:25.660066Z",
     "iopub.status.busy": "2023-04-10T16:55:25.659577Z",
     "iopub.status.idle": "2023-04-10T16:55:31.045642Z",
     "shell.execute_reply": "2023-04-10T16:55:31.044064Z",
     "shell.execute_reply.started": "2023-04-10T16:55:25.660026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search Output:\n",
      "Input sentence: implement a system to reduce waste and increase efficiencies\n",
      "\n",
      "Translation: రీసైక్లింగ్ ను ప్రోత్సహించండి చాలా దేశాల్లో ఇప్పటికీ సమర్థవంతమైన రీసైక్లింగ్ కార్యక్రమాలు లేవు. ప్రభుత్వాలు పన్ను మినహాయింపులు రీసైక్లింగ్ నిధులు మరియు రీసైక్లింగ్ నిధులు\n",
      "\n",
      "\n",
      "Beam Search Output:\n",
      "Input sentence: implement a system to reduce waste and increase efficiencies\n",
      "\n",
      "Translation: రీసైక్లింగ్ ను ప్రోత్సహించండి చాలా రీసైక్లింగ్ మిశ్రమం యొక్క ప్రధాన ఆర్\n"
     ]
    }
   ],
   "source": [
    "t = \"implement a system to reduce waste and increase efficiencies\"\n",
    "\n",
    "print(f\"Greedy Search Output:\")\n",
    "translate(t)\n",
    "print(f\"\\n\\nBeam Search Output:\")\n",
    "translate_beam(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
